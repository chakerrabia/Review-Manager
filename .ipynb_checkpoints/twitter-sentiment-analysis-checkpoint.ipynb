{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "92b885dd147dac19bd0a33db3cd0da100bd5bc23"
   },
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "303e72966af732ddef0bd8108a321095314e44af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\skrzy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from keras import utils\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "\n",
    "# Word2vec\n",
    "import gensim\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "# Set log\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "35e1a89dead5fd160e4c9a024a21d2e569fc89ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\skrzy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8b01a07df001e4abcc745900336c4db06e455f3"
   },
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "180f0dd2a95419e4602b5c0229822b0111c826f6"
   },
   "outputs": [],
   "source": [
    "# DATASET\n",
    "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# TEXT CLENAING\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "# WORD2VEC \n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10\n",
    "\n",
    "# KERAS\n",
    "SEQUENCE_LENGTH = 300\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# SENTIMENT\n",
    "POSITIVE = \"POSITIVE\"\n",
    "NEGATIVE = \"NEGATIVE\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
    "\n",
    "# EXPORT\n",
    "KERAS_MODEL = \"model.h5\"\n",
    "WORD2VEC_MODEL = \"model.w2v\"\n",
    "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
    "ENCODER_MODEL = \"encoder.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1c3beecc618be68480b3d4f0de08d9d863da1dc1"
   },
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "563b3c44f1092dba0b853747b098e00509098cca"
   },
   "source": [
    "### Dataset details\n",
    "* **target**: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "* **ids**: The id of the tweet ( 2087)\n",
    "* **date**: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "* **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "* **user**: the user that tweeted (robotickilldozr)\n",
    "* **text**: the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "bba8f91cd70de4f5ea0fb0870ae2029b6e3dcc24"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding =DATASET_ENCODING , names=DATASET_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "936d499c00c4f1648bc16ca9d283c3b39be7fb10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1600000\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "7486ed895b813c5246f97b31b6162b0f65ff763b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3f9a7bb129e184967b13261fb5d253af451c75c5"
   },
   "source": [
    "### Map target label to String\n",
    "* **0** -> **NEGATIVE**\n",
    "* **2** -> **NEUTRAL**\n",
    "* **4** -> **POSITIVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "14074b59106cb9550440839e48b832223fc9502f"
   },
   "outputs": [],
   "source": [
    "decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\n",
    "def decode_sentiment(label):\n",
    "    return decode_map[int(label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "4449d473187f647a195a6ac6986b009da32a7f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.target = df.target.apply(lambda x: decode_sentiment(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4329b1573518b03e497213efa7676220734ebb4b"
   },
   "source": [
    "### Pre-Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "8aeee8b7b9ea11b749c7f91cd4787a7b50ed1a91"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "649ebcb97969b9ac4301138783704bb3d7846a49"
   },
   "outputs": [],
   "source": [
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "f7f3e77ab9291d14687c49e71ba9b2b1e3323432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.text = df.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5f9714a8507409bbe780eebf2855a33e8e6ba37"
   },
   "source": [
    "### Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "d2b1179c968e3f3910c790ecf0c5b2cbb34b0e68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN size: 1280000\n",
      "TEST size: 320000\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\n",
    "print(\"TRAIN size:\", len(df_train))\n",
    "print(\"TEST size:\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f08a28aab2c3d16d8b9681a7d5d07587153a1cd6"
   },
   "source": [
    "### Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "2461bf564de1b4414841933d0c1d1bee5f5cc5a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "documents = [_text.split() for _text in df_train.text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "8e19b9f25801ba86420decc266d2b3e6fb44f1ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-14 18:39:29,701 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=300, alpha=0.025)', 'datetime': '2021-05-14T18:39:29.700694', 'gensim': '4.0.1', 'python': '3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model = gensim.models.word2vec.Word2Vec(vector_size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "58d655af07653c594bec6bebcfb302a973b0ad9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-14 18:39:35,749 : INFO : collecting all words and their counts\n",
      "2021-05-14 18:39:35,751 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-05-14 18:39:35,794 : INFO : PROGRESS: at sentence #10000, processed 72565 words, keeping 14005 word types\n",
      "2021-05-14 18:39:35,853 : INFO : PROGRESS: at sentence #20000, processed 144393 words, keeping 21587 word types\n",
      "2021-05-14 18:39:35,924 : INFO : PROGRESS: at sentence #30000, processed 215826 words, keeping 27541 word types\n",
      "2021-05-14 18:39:35,993 : INFO : PROGRESS: at sentence #40000, processed 288271 words, keeping 32764 word types\n",
      "2021-05-14 18:39:36,071 : INFO : PROGRESS: at sentence #50000, processed 359772 words, keeping 37587 word types\n",
      "2021-05-14 18:39:36,134 : INFO : PROGRESS: at sentence #60000, processed 431431 words, keeping 42198 word types\n",
      "2021-05-14 18:39:36,214 : INFO : PROGRESS: at sentence #70000, processed 503103 words, keeping 46458 word types\n",
      "2021-05-14 18:39:36,275 : INFO : PROGRESS: at sentence #80000, processed 575709 words, keeping 50476 word types\n",
      "2021-05-14 18:39:36,344 : INFO : PROGRESS: at sentence #90000, processed 647100 words, keeping 54140 word types\n",
      "2021-05-14 18:39:36,432 : INFO : PROGRESS: at sentence #100000, processed 718681 words, keeping 57777 word types\n",
      "2021-05-14 18:39:36,499 : INFO : PROGRESS: at sentence #110000, processed 790696 words, keeping 61207 word types\n",
      "2021-05-14 18:39:36,553 : INFO : PROGRESS: at sentence #120000, processed 863134 words, keeping 64583 word types\n",
      "2021-05-14 18:39:36,592 : INFO : PROGRESS: at sentence #130000, processed 935111 words, keeping 67865 word types\n",
      "2021-05-14 18:39:36,628 : INFO : PROGRESS: at sentence #140000, processed 1006668 words, keeping 70966 word types\n",
      "2021-05-14 18:39:36,674 : INFO : PROGRESS: at sentence #150000, processed 1078512 words, keeping 74119 word types\n",
      "2021-05-14 18:39:36,728 : INFO : PROGRESS: at sentence #160000, processed 1149914 words, keeping 77187 word types\n",
      "2021-05-14 18:39:36,777 : INFO : PROGRESS: at sentence #170000, processed 1222145 words, keeping 80267 word types\n",
      "2021-05-14 18:39:36,866 : INFO : PROGRESS: at sentence #180000, processed 1294708 words, keeping 83393 word types\n",
      "2021-05-14 18:39:36,963 : INFO : PROGRESS: at sentence #190000, processed 1367608 words, keeping 86329 word types\n",
      "2021-05-14 18:39:37,051 : INFO : PROGRESS: at sentence #200000, processed 1439469 words, keeping 89103 word types\n",
      "2021-05-14 18:39:37,100 : INFO : PROGRESS: at sentence #210000, processed 1512099 words, keeping 91840 word types\n",
      "2021-05-14 18:39:37,148 : INFO : PROGRESS: at sentence #220000, processed 1584149 words, keeping 94636 word types\n",
      "2021-05-14 18:39:37,211 : INFO : PROGRESS: at sentence #230000, processed 1656354 words, keeping 97353 word types\n",
      "2021-05-14 18:39:37,305 : INFO : PROGRESS: at sentence #240000, processed 1728573 words, keeping 99975 word types\n",
      "2021-05-14 18:39:37,369 : INFO : PROGRESS: at sentence #250000, processed 1801102 words, keeping 102594 word types\n",
      "2021-05-14 18:39:37,418 : INFO : PROGRESS: at sentence #260000, processed 1873103 words, keeping 105162 word types\n",
      "2021-05-14 18:39:37,468 : INFO : PROGRESS: at sentence #270000, processed 1945245 words, keeping 107626 word types\n",
      "2021-05-14 18:39:37,578 : INFO : PROGRESS: at sentence #280000, processed 2017163 words, keeping 110141 word types\n",
      "2021-05-14 18:39:37,650 : INFO : PROGRESS: at sentence #290000, processed 2089574 words, keeping 112539 word types\n",
      "2021-05-14 18:39:37,728 : INFO : PROGRESS: at sentence #300000, processed 2160996 words, keeping 114893 word types\n",
      "2021-05-14 18:39:37,794 : INFO : PROGRESS: at sentence #310000, processed 2232913 words, keeping 117298 word types\n",
      "2021-05-14 18:39:37,869 : INFO : PROGRESS: at sentence #320000, processed 2305039 words, keeping 119693 word types\n",
      "2021-05-14 18:39:37,943 : INFO : PROGRESS: at sentence #330000, processed 2377119 words, keeping 122131 word types\n",
      "2021-05-14 18:39:38,010 : INFO : PROGRESS: at sentence #340000, processed 2449370 words, keeping 124416 word types\n",
      "2021-05-14 18:39:38,075 : INFO : PROGRESS: at sentence #350000, processed 2521564 words, keeping 126669 word types\n",
      "2021-05-14 18:39:38,142 : INFO : PROGRESS: at sentence #360000, processed 2593681 words, keeping 128912 word types\n",
      "2021-05-14 18:39:38,209 : INFO : PROGRESS: at sentence #370000, processed 2665692 words, keeping 131135 word types\n",
      "2021-05-14 18:39:38,273 : INFO : PROGRESS: at sentence #380000, processed 2737859 words, keeping 133403 word types\n",
      "2021-05-14 18:39:38,330 : INFO : PROGRESS: at sentence #390000, processed 2809848 words, keeping 135551 word types\n",
      "2021-05-14 18:39:38,398 : INFO : PROGRESS: at sentence #400000, processed 2882438 words, keeping 137742 word types\n",
      "2021-05-14 18:39:38,462 : INFO : PROGRESS: at sentence #410000, processed 2954075 words, keeping 139909 word types\n",
      "2021-05-14 18:39:38,533 : INFO : PROGRESS: at sentence #420000, processed 3026247 words, keeping 142144 word types\n",
      "2021-05-14 18:39:38,592 : INFO : PROGRESS: at sentence #430000, processed 3098659 words, keeping 144364 word types\n",
      "2021-05-14 18:39:38,656 : INFO : PROGRESS: at sentence #440000, processed 3170663 words, keeping 146439 word types\n",
      "2021-05-14 18:39:38,702 : INFO : PROGRESS: at sentence #450000, processed 3243344 words, keeping 148526 word types\n",
      "2021-05-14 18:39:38,735 : INFO : PROGRESS: at sentence #460000, processed 3315466 words, keeping 150610 word types\n",
      "2021-05-14 18:39:38,773 : INFO : PROGRESS: at sentence #470000, processed 3388295 words, keeping 152737 word types\n",
      "2021-05-14 18:39:38,809 : INFO : PROGRESS: at sentence #480000, processed 3460120 words, keeping 154757 word types\n",
      "2021-05-14 18:39:38,853 : INFO : PROGRESS: at sentence #490000, processed 3531883 words, keeping 156825 word types\n",
      "2021-05-14 18:39:38,904 : INFO : PROGRESS: at sentence #500000, processed 3604217 words, keeping 158859 word types\n",
      "2021-05-14 18:39:38,983 : INFO : PROGRESS: at sentence #510000, processed 3676427 words, keeping 160852 word types\n",
      "2021-05-14 18:39:39,048 : INFO : PROGRESS: at sentence #520000, processed 3749045 words, keeping 162863 word types\n",
      "2021-05-14 18:39:39,110 : INFO : PROGRESS: at sentence #530000, processed 3821622 words, keeping 164929 word types\n",
      "2021-05-14 18:39:39,189 : INFO : PROGRESS: at sentence #540000, processed 3893627 words, keeping 166840 word types\n",
      "2021-05-14 18:39:39,248 : INFO : PROGRESS: at sentence #550000, processed 3965477 words, keeping 168799 word types\n",
      "2021-05-14 18:39:39,298 : INFO : PROGRESS: at sentence #560000, processed 4038050 words, keeping 170802 word types\n",
      "2021-05-14 18:39:39,353 : INFO : PROGRESS: at sentence #570000, processed 4110296 words, keeping 172760 word types\n",
      "2021-05-14 18:39:39,400 : INFO : PROGRESS: at sentence #580000, processed 4182385 words, keeping 174635 word types\n",
      "2021-05-14 18:39:39,452 : INFO : PROGRESS: at sentence #590000, processed 4254632 words, keeping 176470 word types\n",
      "2021-05-14 18:39:39,486 : INFO : PROGRESS: at sentence #600000, processed 4326859 words, keeping 178350 word types\n",
      "2021-05-14 18:39:39,540 : INFO : PROGRESS: at sentence #610000, processed 4399183 words, keeping 180290 word types\n",
      "2021-05-14 18:39:39,638 : INFO : PROGRESS: at sentence #620000, processed 4471343 words, keeping 182129 word types\n",
      "2021-05-14 18:39:39,706 : INFO : PROGRESS: at sentence #630000, processed 4543286 words, keeping 184005 word types\n",
      "2021-05-14 18:39:39,774 : INFO : PROGRESS: at sentence #640000, processed 4615780 words, keeping 185835 word types\n",
      "2021-05-14 18:39:39,843 : INFO : PROGRESS: at sentence #650000, processed 4688481 words, keeping 187705 word types\n",
      "2021-05-14 18:39:39,906 : INFO : PROGRESS: at sentence #660000, processed 4760481 words, keeping 189439 word types\n",
      "2021-05-14 18:39:39,973 : INFO : PROGRESS: at sentence #670000, processed 4833024 words, keeping 191232 word types\n",
      "2021-05-14 18:39:40,041 : INFO : PROGRESS: at sentence #680000, processed 4904516 words, keeping 193177 word types\n",
      "2021-05-14 18:39:40,101 : INFO : PROGRESS: at sentence #690000, processed 4976968 words, keeping 194960 word types\n",
      "2021-05-14 18:39:40,176 : INFO : PROGRESS: at sentence #700000, processed 5049412 words, keeping 196725 word types\n",
      "2021-05-14 18:39:40,248 : INFO : PROGRESS: at sentence #710000, processed 5121976 words, keeping 198516 word types\n",
      "2021-05-14 18:39:40,312 : INFO : PROGRESS: at sentence #720000, processed 5193881 words, keeping 200325 word types\n",
      "2021-05-14 18:39:40,381 : INFO : PROGRESS: at sentence #730000, processed 5265467 words, keeping 202133 word types\n",
      "2021-05-14 18:39:40,452 : INFO : PROGRESS: at sentence #740000, processed 5337518 words, keeping 203818 word types\n",
      "2021-05-14 18:39:40,520 : INFO : PROGRESS: at sentence #750000, processed 5409321 words, keeping 205535 word types\n",
      "2021-05-14 18:39:40,590 : INFO : PROGRESS: at sentence #760000, processed 5481512 words, keeping 207282 word types\n",
      "2021-05-14 18:39:40,629 : INFO : PROGRESS: at sentence #770000, processed 5554093 words, keeping 209076 word types\n",
      "2021-05-14 18:39:40,666 : INFO : PROGRESS: at sentence #780000, processed 5625382 words, keeping 210805 word types\n",
      "2021-05-14 18:39:40,705 : INFO : PROGRESS: at sentence #790000, processed 5698066 words, keeping 212618 word types\n",
      "2021-05-14 18:39:40,763 : INFO : PROGRESS: at sentence #800000, processed 5770880 words, keeping 214374 word types\n",
      "2021-05-14 18:39:40,854 : INFO : PROGRESS: at sentence #810000, processed 5843418 words, keeping 216009 word types\n",
      "2021-05-14 18:39:40,895 : INFO : PROGRESS: at sentence #820000, processed 5915628 words, keeping 217804 word types\n",
      "2021-05-14 18:39:40,934 : INFO : PROGRESS: at sentence #830000, processed 5987499 words, keeping 219585 word types\n",
      "2021-05-14 18:39:40,969 : INFO : PROGRESS: at sentence #840000, processed 6058973 words, keeping 221344 word types\n",
      "2021-05-14 18:39:41,004 : INFO : PROGRESS: at sentence #850000, processed 6131125 words, keeping 223002 word types\n",
      "2021-05-14 18:39:41,093 : INFO : PROGRESS: at sentence #860000, processed 6202951 words, keeping 224643 word types\n",
      "2021-05-14 18:39:41,155 : INFO : PROGRESS: at sentence #870000, processed 6275461 words, keeping 226362 word types\n",
      "2021-05-14 18:39:41,208 : INFO : PROGRESS: at sentence #880000, processed 6347661 words, keeping 227986 word types\n",
      "2021-05-14 18:39:41,249 : INFO : PROGRESS: at sentence #890000, processed 6419806 words, keeping 229634 word types\n",
      "2021-05-14 18:39:41,280 : INFO : PROGRESS: at sentence #900000, processed 6491644 words, keeping 231389 word types\n",
      "2021-05-14 18:39:41,314 : INFO : PROGRESS: at sentence #910000, processed 6564022 words, keeping 233050 word types\n",
      "2021-05-14 18:39:41,348 : INFO : PROGRESS: at sentence #920000, processed 6636228 words, keeping 234686 word types\n",
      "2021-05-14 18:39:41,378 : INFO : PROGRESS: at sentence #930000, processed 6708573 words, keeping 236393 word types\n",
      "2021-05-14 18:39:41,411 : INFO : PROGRESS: at sentence #940000, processed 6779956 words, keeping 238052 word types\n",
      "2021-05-14 18:39:41,479 : INFO : PROGRESS: at sentence #950000, processed 6852599 words, keeping 239716 word types\n",
      "2021-05-14 18:39:41,589 : INFO : PROGRESS: at sentence #960000, processed 6924717 words, keeping 241354 word types\n",
      "2021-05-14 18:39:41,679 : INFO : PROGRESS: at sentence #970000, processed 6996992 words, keeping 242980 word types\n",
      "2021-05-14 18:39:41,748 : INFO : PROGRESS: at sentence #980000, processed 7068402 words, keeping 244646 word types\n",
      "2021-05-14 18:39:41,822 : INFO : PROGRESS: at sentence #990000, processed 7140346 words, keeping 246186 word types\n",
      "2021-05-14 18:39:41,888 : INFO : PROGRESS: at sentence #1000000, processed 7211757 words, keeping 247726 word types\n",
      "2021-05-14 18:39:41,951 : INFO : PROGRESS: at sentence #1010000, processed 7283267 words, keeping 249288 word types\n",
      "2021-05-14 18:39:42,020 : INFO : PROGRESS: at sentence #1020000, processed 7355299 words, keeping 250860 word types\n",
      "2021-05-14 18:39:42,085 : INFO : PROGRESS: at sentence #1030000, processed 7426918 words, keeping 252366 word types\n",
      "2021-05-14 18:39:42,155 : INFO : PROGRESS: at sentence #1040000, processed 7498815 words, keeping 253930 word types\n",
      "2021-05-14 18:39:42,220 : INFO : PROGRESS: at sentence #1050000, processed 7570499 words, keeping 255471 word types\n",
      "2021-05-14 18:39:42,290 : INFO : PROGRESS: at sentence #1060000, processed 7643251 words, keeping 257035 word types\n",
      "2021-05-14 18:39:42,365 : INFO : PROGRESS: at sentence #1070000, processed 7714721 words, keeping 258509 word types\n",
      "2021-05-14 18:39:42,448 : INFO : PROGRESS: at sentence #1080000, processed 7787371 words, keeping 260071 word types\n",
      "2021-05-14 18:39:42,514 : INFO : PROGRESS: at sentence #1090000, processed 7859336 words, keeping 261683 word types\n",
      "2021-05-14 18:39:42,579 : INFO : PROGRESS: at sentence #1100000, processed 7932029 words, keeping 263278 word types\n",
      "2021-05-14 18:39:42,614 : INFO : PROGRESS: at sentence #1110000, processed 8004146 words, keeping 264800 word types\n",
      "2021-05-14 18:39:42,645 : INFO : PROGRESS: at sentence #1120000, processed 8075880 words, keeping 266309 word types\n",
      "2021-05-14 18:39:42,679 : INFO : PROGRESS: at sentence #1130000, processed 8148163 words, keeping 267826 word types\n",
      "2021-05-14 18:39:42,720 : INFO : PROGRESS: at sentence #1140000, processed 8220487 words, keeping 269391 word types\n",
      "2021-05-14 18:39:42,761 : INFO : PROGRESS: at sentence #1150000, processed 8292498 words, keeping 270894 word types\n",
      "2021-05-14 18:39:42,805 : INFO : PROGRESS: at sentence #1160000, processed 8363838 words, keeping 272400 word types\n",
      "2021-05-14 18:39:42,853 : INFO : PROGRESS: at sentence #1170000, processed 8435510 words, keeping 273970 word types\n",
      "2021-05-14 18:39:42,948 : INFO : PROGRESS: at sentence #1180000, processed 8507795 words, keeping 275521 word types\n",
      "2021-05-14 18:39:43,022 : INFO : PROGRESS: at sentence #1190000, processed 8579080 words, keeping 277007 word types\n",
      "2021-05-14 18:39:43,087 : INFO : PROGRESS: at sentence #1200000, processed 8650606 words, keeping 278457 word types\n",
      "2021-05-14 18:39:43,138 : INFO : PROGRESS: at sentence #1210000, processed 8721893 words, keeping 279959 word types\n",
      "2021-05-14 18:39:43,201 : INFO : PROGRESS: at sentence #1220000, processed 8793795 words, keeping 281427 word types\n",
      "2021-05-14 18:39:43,259 : INFO : PROGRESS: at sentence #1230000, processed 8865726 words, keeping 282981 word types\n",
      "2021-05-14 18:39:43,310 : INFO : PROGRESS: at sentence #1240000, processed 8938173 words, keeping 284542 word types\n",
      "2021-05-14 18:39:43,364 : INFO : PROGRESS: at sentence #1250000, processed 9010842 words, keeping 286064 word types\n",
      "2021-05-14 18:39:43,457 : INFO : PROGRESS: at sentence #1260000, processed 9083261 words, keeping 287521 word types\n",
      "2021-05-14 18:39:43,530 : INFO : PROGRESS: at sentence #1270000, processed 9155616 words, keeping 288987 word types\n",
      "2021-05-14 18:39:43,588 : INFO : collected 290418 word types from a corpus of 9227204 raw words and 1280000 sentences\n",
      "2021-05-14 18:39:43,590 : INFO : Creating a fresh vocabulary\n",
      "2021-05-14 18:39:44,572 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 30369 unique words (10.456996467161126%% of original 290418, drops 260049)', 'datetime': '2021-05-14T18:39:44.572495', 'gensim': '4.0.1', 'python': '3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-05-14 18:39:44,576 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 8780739 word corpus (95.16142701516083%% of original 9227204, drops 446465)', 'datetime': '2021-05-14T18:39:44.576466', 'gensim': '4.0.1', 'python': '3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-05-14 18:39:45,324 : INFO : deleting the raw counts dictionary of 290418 items\n",
      "2021-05-14 18:39:45,340 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2021-05-14 18:39:45,342 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 8222658.616429881 word corpus (93.6%% of prior 8780739)', 'datetime': '2021-05-14T18:39:45.342753', 'gensim': '4.0.1', 'python': '3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-05-14 18:39:46,473 : INFO : estimated required memory for 30369 words and 300 dimensions: 88070100 bytes\n",
      "2021-05-14 18:39:46,476 : INFO : resetting layer weights\n",
      "2021-05-14 18:39:46,628 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-05-14T18:39:46.628029', 'gensim': '4.0.1', 'python': '3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "72a5628ca81fd4b8983c12d93ae0bf950b86b6ae"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-43ef4061ac6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vocab size\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "words = w2v_model.wv.vocab.keys()\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68c3e4a5ba07cac3dee67f78ecdd1404c7f83f14"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27cc2651c74227115d8bfd8c40e5618048e05edd"
   },
   "outputs": [],
   "source": [
    "w2v_model.most_similar(\"love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e13563644468037258598637b49373ca96b9b879"
   },
   "source": [
    "### Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6852bc709a7cd20173cbeeb218505078f8f37c57"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train.text)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Total words\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45de439df3015030c71f84c2d170346936a1d68f"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "03b35903fc6260e190d6928d240ef7432de117fc"
   },
   "source": [
    "### Label Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33676e0efa39e97d89bd650b8b4eae933a22fbf0"
   },
   "outputs": [],
   "source": [
    "labels = df_train.target.unique().tolist()\n",
    "labels.append(NEUTRAL)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04239a9bef76e7922fd86098a5601dfde8ee4665"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(df_train.target.tolist())\n",
    "\n",
    "y_train = encoder.transform(df_train.target.tolist())\n",
    "y_test = encoder.transform(df_test.target.tolist())\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"y_test\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04299c886911ca135583ab64878f213939a2990c"
   },
   "outputs": [],
   "source": [
    "print(\"x_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print()\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "232533fb27b7be99d9b8c2f8fb22c9c6bf121a6f"
   },
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "233c0ea94055a03e2e7df3e2a13d036ec963484f"
   },
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ab488374b59e3f30f8b1ea92767d853c4846bac"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "  if word in w2v_model.wv:\n",
    "    embedding_matrix[i] = w2v_model.wv[word]\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "833279d91e4286065968237fb5f2a0c2dd4d246c"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b299ef78f94c2085942c993a2d58753a7476305a"
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e775ef4f1b74e6412457181383c39f2df554ef3f"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28d22eafd0c7d798dcf3d742bc92fb8577939e6c"
   },
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1331e08d590bb2aa2033706c8faca217afc0f1c3"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7733127cb8b380e0c807268903bf4d03ef92542"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a688df590386f5748da6fe00b01904fe6c71619e"
   },
   "outputs": [],
   "source": [
    "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
    "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8d0873633dd49179c8cae17377641b97d323ef3b"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b659d390c6577dc5cdb6b6297934279b4e801d5"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "267258196d96796ac69a7b8c466314bcf5d6ee42"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98ecd8f1b8b74594c3ea775dd68a094e92458022"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print()\n",
    "print(\"ACCURACY:\",score[1])\n",
    "print(\"LOSS:\",score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6bdfc0f6a6af5bebc0271d83dd7432c91001409b"
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f0b0fa3d4b1bb14b3f5e3d169a369f3ebef29ae1"
   },
   "outputs": [],
   "source": [
    "def decode_sentiment(score, include_neutral=True):\n",
    "    if include_neutral:        \n",
    "        label = NEUTRAL\n",
    "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
    "            label = NEGATIVE\n",
    "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
    "            label = POSITIVE\n",
    "\n",
    "        return label\n",
    "    else:\n",
    "        return NEGATIVE if score < 0.5 else POSITIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ed4086d651f2f8cbed11d3c909a8873607d29a06"
   },
   "outputs": [],
   "source": [
    "def predict(text, include_neutral=True):\n",
    "    start_at = time.time()\n",
    "    # Tokenize text\n",
    "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
    "    # Predict\n",
    "    score = model.predict([x_test])[0]\n",
    "    # Decode sentiment\n",
    "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
    "\n",
    "    return {\"label\": label, \"score\": float(score),\n",
    "       \"elapsed_time\": time.time()-start_at}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca38b1e6c9b5acfed7467de2cf02a78333108872"
   },
   "outputs": [],
   "source": [
    "predict(\"I love the music\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e5fe647533be0148850de349fea6ef6f71303d1"
   },
   "outputs": [],
   "source": [
    "predict(\"I hate the rain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "37064dffcc8920d34ccd54fac7c8b50e583a8269"
   },
   "outputs": [],
   "source": [
    "predict(\"i don't know what i'm doing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ee72e47f84b6dbc32e02a783de5ec1661f157e1"
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e920173eb05f04aecdd735bc5dff0f5be5f8d15"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred_1d = []\n",
    "y_test_1d = list(df_test.target)\n",
    "scores = model.predict(x_test, verbose=1, batch_size=8000)\n",
    "y_pred_1d = [decode_sentiment(score, include_neutral=False) for score in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e23b957348dcc084249d3cc7538b972da471c2cd"
   },
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7fe05b7caa1c984ff1deb0be2f7c6bc043df9f5"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test_1d, y_pred_1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4eb300f0c6693a618587c7dcf32f77f5416cbfb9"
   },
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5cf76e6e09f8a60ed25947932b94c772eda44d23"
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test_1d, y_pred_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f014c32f3833db282e1a075c526604f34e3158c"
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b2b3ad5b592977b404acfa1c9ad303a62837255"
   },
   "outputs": [],
   "source": [
    "model.save(KERAS_MODEL)\n",
    "w2v_model.save(WORD2VEC_MODEL)\n",
    "pickle.dump(tokenizer, open(TOKENIZER_MODEL, \"wb\"), protocol=0)\n",
    "pickle.dump(encoder, open(ENCODER_MODEL, \"wb\"), protocol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc363c54782894757f5ea8820c6a170f2e16ef93"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
