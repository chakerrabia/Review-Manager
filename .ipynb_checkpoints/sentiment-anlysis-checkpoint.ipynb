{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and EDA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\skrzy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                     timestamp     query  \\\n",
       "1599995       1  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       1  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       1  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       1  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       1  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data, setting columns and initial expoloration\n",
    "\n",
    "columns = ['target', 'id','timestamp','query','user','tweet']\n",
    "tweet_data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",header = None, names = columns, encoding='latin-1')\n",
    "tweet_data['target'][tweet_data['target']==4]=1\n",
    "tweet_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   target     1600000 non-null  int64 \n",
      " 1   id         1600000 non-null  int64 \n",
      " 2   timestamp  1600000 non-null  object\n",
      " 3   query      1600000 non-null  object\n",
      " 4   user       1600000 non-null  object\n",
      " 5   tweet      1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Exploring data\n",
    "tweet_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null values\n",
    "np.sum(tweet_data.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                     timestamp     query  \\\n",
       "1599995       1  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       1  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       1  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       1  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       1  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample choice: 20000 negative and positive tweets\n",
    "neg = tweet_data[tweet_data['target'] == 0]\n",
    "pos = tweet_data[tweet_data['target'] == 1]\n",
    "pos.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = neg.iloc[:int(20000)]\n",
    "pos = pos.iloc[:int(20000)]\n",
    "tweet_data = pd.concat([pos, neg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing \n",
    "### Steps to follow: \n",
    "**Uncaps text**\n",
    "\n",
    "**Remove stopwords**\n",
    "\n",
    "**Remove puncuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['tweet']=tweet_data['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000          i love @health4uandpets u guys r the best! \n",
       "800001    im meting up with one of my besties tonight! c...\n",
       "800002    @darealsunisakim thanks for the twiter ad, sun...\n",
       "800003    being sick can be realy cheap when it hurts to...\n",
       "800004        @lovesbroklyn2 he has that efect on everyone \n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def remove_repeated_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "tweet_data['tweet'] = tweet_data['tweet'].apply(lambda tweet :remove_repeated_char(tweet))\n",
    "\n",
    "tweet_data['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                 love @health4uandpets u guys r best!\n",
       "800001    im meting one besties tonight! cant wait! - gi...\n",
       "800002    @darealsunisakim thanks twiter ad, sunisa! got...\n",
       "800003    sick realy cheap hurts much eat real fod plus,...\n",
       "800004                        @lovesbroklyn2 efect everyone\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stopwords_list])\n",
    "\n",
    "tweet_data['tweet'] = tweet_data['tweet'].apply(lambda tweet :remove_stopwords(tweet))\n",
    "\n",
    "tweet_data['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                   love health4uandpets u guys r best\n",
       "800001    im meting one besties tonight cant wait  girl ...\n",
       "800002    darealsunisakim thanks twiter ad sunisa got me...\n",
       "800003    sick realy cheap hurts much eat real fod plus ...\n",
       "800004                         lovesbroklyn2 efect everyone\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punctuations_list = string.punctuation\n",
    "\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "tweet_data['tweet'] = tweet_data['tweet'].apply(lambda tweet :remove_punct(tweet))\n",
    "\n",
    "tweet_data['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000                    love healthuandpets u guys r best\n",
       "800001    im meting one besties tonight cant wait  girl ...\n",
       "800002    darealsunisakim thanks twiter ad sunisa got me...\n",
       "800003    sick realy cheap hurts much eat real fod plus ...\n",
       "800004                          lovesbroklyn efect everyone\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing @ signs and URLs \n",
    "\n",
    "def remove_URLs(text):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
    "\n",
    "def remove_numeric(text):\n",
    "    return  re.sub('[0-9]+', '', text)\n",
    "\n",
    "\n",
    "tweet_data['tweet'] = tweet_data['tweet'].apply(lambda tweet: remove_URLs(tweet) )\n",
    "tweet_data['tweet'] = tweet_data['tweet'].apply(lambda tweet: remove_numeric(tweet) )\n",
    "\n",
    "tweet_data['tweet'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000             [love, healthuandpets, u, guys, r, best]\n",
       "800001    [im, meting, one, besties, tonight, cant, wait...\n",
       "800002    [darealsunisakim, thanks, twiter, ad, sunisa, ...\n",
       "800003    [sick, realy, cheap, hurts, much, eat, real, f...\n",
       "800004                      [lovesbroklyn, efect, everyone]\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_tweet(text):\n",
    "    words = word_tokenize(text)\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "tweet_data['tweet'] = tweet_data['tweet'].apply(lambda tweet: tokenize_tweet(tweet))\n",
    "\n",
    "tweet_data['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tweet(tokens):\n",
    "    data = [lemmatizer.lemmatize(word) for word in tokens ]\n",
    "    return data\n",
    "\n",
    "tweet_data['tweet']= tweet_data['tweet'].apply(lambda tweet : lemmatize_tweet(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000              [love, healthuandpets, u, guy, r, best]\n",
       "800001    [im, meting, one, besties, tonight, cant, wait...\n",
       "800002    [darealsunisakim, thanks, twiter, ad, sunisa, ...\n",
       "800003    [sick, realy, cheap, hurt, much, eat, real, fo...\n",
       "800004                      [lovesbroklyn, efect, everyone]\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the finalized preprocessed tweets\n",
    "tweet_data['tweet'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training preparaion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating labels & tweets \n",
    "X = tweet_data.tweet\n",
    "Y = tweet_data.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_len = 500\n",
    "tok = Tokenizer(num_words=2000)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Test Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, Y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Now that preprocessing is out of the way, it is now time to train the model, test it and calibrate. ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "# Model Definition \n",
    "def tensorflow_based_model(): #Defined tensorflow_based_model function for training tenforflow based model\n",
    "    inputs = Input(name='inputs',shape=[max_len])#step1\n",
    "    layer = Embedding(2000,50,input_length=max_len)(inputs) #step2\n",
    "    layer = LSTM(64)(layer) #step3\n",
    "    layer = Dense(256,name='FC1')(layer) #step4\n",
    "    layer = Activation('relu')(layer) # step5\n",
    "    layer = Dropout(0.5)(layer) # step6\n",
    "    layer = Dense(1,name='out_layer')(layer) #step4 again but this time its giving only one output as because we need to classify the tweet as positive or negative\n",
    "    layer = Activation('sigmoid')(layer) #step5 but this time activation function is sigmoid for only one output.\n",
    "    model = Model(inputs=inputs,outputs=layer) #here we are getting the final output value in the model for classification\n",
    "    return model #function returning the value when we call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tensorflow_based_model()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "315/315 [==============================] - 242s 739ms/step - loss: 0.6252 - accuracy: 0.6336 - val_loss: 0.5250 - val_accuracy: 0.7329\n",
      "Epoch 2/8\n",
      "315/315 [==============================] - 231s 734ms/step - loss: 0.5091 - accuracy: 0.7558 - val_loss: 0.5231 - val_accuracy: 0.7368\n",
      "Epoch 3/8\n",
      "315/315 [==============================] - 203s 644ms/step - loss: 0.4897 - accuracy: 0.7651 - val_loss: 0.5283 - val_accuracy: 0.7357\n",
      "Epoch 4/8\n",
      "315/315 [==============================] - 190s 604ms/step - loss: 0.4815 - accuracy: 0.7708 - val_loss: 0.5260 - val_accuracy: 0.7375\n",
      "Epoch 5/8\n",
      "315/315 [==============================] - 192s 609ms/step - loss: 0.4665 - accuracy: 0.7810 - val_loss: 0.5281 - val_accuracy: 0.7346\n",
      "Epoch 6/8\n",
      "315/315 [==============================] - 195s 620ms/step - loss: 0.4668 - accuracy: 0.7762 - val_loss: 0.5460 - val_accuracy: 0.7386\n",
      "Epoch 7/8\n",
      "315/315 [==============================] - 226s 718ms/step - loss: 0.4423 - accuracy: 0.7876 - val_loss: 0.6516 - val_accuracy: 0.5400\n",
      "Epoch 8/8\n",
      "315/315 [==============================] - 205s 651ms/step - loss: 0.4461 - accuracy: 0.7802 - val_loss: 0.5584 - val_accuracy: 0.7307\n",
      "done training\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,Y_train,batch_size=80,epochs=8, validation_split=0.1)\n",
    "print(\"done training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 38s 99ms/step - loss: 0.5390 - accuracy: 0.7415\n",
      "Test set\n",
      "  Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "accr1 = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Accuracy: {:0.2f}'.format(accr1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: assets\n"
     ]
    }
   ],
   "source": [
    "model.save('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-83088c8392a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msequences_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "sequences_matrix.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 500)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stopwords_list])\n",
    "\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_URLs(text):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
    "\n",
    "def remove_numeric(text):\n",
    "    return  re.sub('[0-9]+', '', text)\n",
    "\n",
    "def tokenize_message(text):\n",
    "    words = word_tokenize(text)\n",
    "    return text.split()\n",
    "\n",
    "def lemmatize_message(tokens):\n",
    "    data = [lemmatizer.lemmatize(word) for word in tokens ]\n",
    "    return data\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punctuations_list = string.punctuation\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "feedback='very bad terrible product. buggy product needs a lot of fixes. incredibly bad and terrible product. the latest update completely broke the entire app and made it unusable'\n",
    "\n",
    "data =  pd.DataFrame({'message': [feedback]})\n",
    "data['message'].loc[0].lower()\n",
    "\n",
    "re.sub(r'(.)\\1+', r'\\1', data['message'].loc[0])\n",
    "data['message'].loc[0] = remove_stopwords(data['message'].loc[0])\n",
    "data['message'].loc[0] = remove_punct(data['message'].loc[0])\n",
    "data['message'].loc[0] = remove_URLs(data['message'].loc[0])\n",
    "data['message'].loc[0] = remove_numeric(data['message'].loc[0])\n",
    "data['message'].loc[0] = tokenize_message(data['message'].loc[0])\n",
    "data['message'].loc[0] = lemmatize_message(data['message'].loc[0])\n",
    "X= data.message\n",
    "\n",
    "max_len = 500\n",
    "tok = Tokenizer(num_words=2000)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "sequences_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43569538]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(sequences_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
